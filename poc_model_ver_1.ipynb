{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3557b59-3f13-492a-9761-f610071bfe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command        object\n",
      "Description    object\n",
      "dtype: object\n",
      "                                 Command  \\\n",
      "0                    -iL <inputfilename>   \n",
      "1                        -iR <num hosts>   \n",
      "2  --exclude <host1[,host2][,host3],...>   \n",
      "3           --excludefile <exclude_file>   \n",
      "4                                    -sL   \n",
      "\n",
      "                                         Description  \n",
      "0  {\"Description\": \"Input from list of hosts/netw...  \n",
      "1  {\"Description\": \"Choose random targets\", \"Deta...  \n",
      "2  {\"Description\": \"Exclude hosts/networks\", \"Det...  \n",
      "3  {\"Description\": \"Exclude list from file\", \"Det...  \n",
      "4  {\"Description\": \"List Scan - simply list targe...  \n"
     ]
    }
   ],
   "source": [
    "# 1. Data Preparation\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load JSON file into a dictionary\n",
    "def load_json_as_dict(json_file_path):\n",
    "    with open(json_file_path, mode='r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = \"nmap_commands_1.json\"\n",
    "\n",
    "# Load the JSON file as a dictionary\n",
    "nmap_dataset = load_json_as_dict(json_file_path)\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(list(nmap_dataset.items()), columns=['Command', 'Description'])\n",
    "\n",
    "# Check data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert 'Description' to string if it's not already\n",
    "df['Description'] = df['Description'].apply(lambda x: json.dumps(x) if isinstance(x, dict) else str(x))\n",
    "\n",
    "# Check data after conversion\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e21b66-a11a-4d38-b5bb-da06d83140bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Command  \\\n",
      "0                    -iL <inputfilename>   \n",
      "1                        -iR <num hosts>   \n",
      "2  --exclude <host1[,host2][,host3],...>   \n",
      "3           --excludefile <exclude_file>   \n",
      "4                                    -sL   \n",
      "\n",
      "                                         Description  \\\n",
      "0  {\"Description\": \"Input from list of hosts/netw...   \n",
      "1  {\"Description\": \"Choose random targets\", \"Deta...   \n",
      "2  {\"Description\": \"Exclude hosts/networks\", \"Det...   \n",
      "3  {\"Description\": \"Exclude list from file\", \"Det...   \n",
      "4  {\"Description\": \"List Scan - simply list targe...   \n",
      "\n",
      "                               Processed_Description  \n",
      "0  description input list detail specify file con...  \n",
      "1  description choose random target detail select...  \n",
      "2  description exclude detail exclude specific ho...  \n",
      "3  description exclude list file detail exclude h...  \n",
      "4  description list scan simply list target scan ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and non-alphabetic tokens, and lemmatize the tokens\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Apply preprocessing to the 'Description' column\n",
    "df['Processed_Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a41ef5-408a-4ef9-8272-9d4ee874a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 1149 stored elements and shape (116, 347)>\n",
      "  Coords\tValues\n",
      "  (0, 63)\t0.08122997999862391\n",
      "  (0, 140)\t0.37882101923746075\n",
      "  (0, 161)\t0.598296557266506\n",
      "  (0, 64)\t0.08122997999862391\n",
      "  (0, 294)\t0.17047023423337693\n",
      "  (0, 98)\t0.32251668762447383\n",
      "  (0, 47)\t0.37882101923746075\n",
      "  (0, 125)\t0.26621235601148696\n",
      "  (0, 178)\t0.32251668762447383\n",
      "  (0, 262)\t0.1840400341892543\n",
      "  (1, 63)\t0.06716434804092647\n",
      "  (1, 64)\t0.06716434804092647\n",
      "  (1, 33)\t0.31322507751096756\n",
      "  (1, 230)\t0.6264501550219351\n",
      "  (1, 312)\t0.5578316150562505\n",
      "  (1, 269)\t0.29390309867168113\n",
      "  (1, 292)\t0.17356074207482236\n",
      "  (1, 186)\t0.2147395042772139\n",
      "  (1, 264)\t0.16320731216415893\n",
      "  (2, 63)\t0.08856967879868427\n",
      "  (2, 64)\t0.08856967879868427\n",
      "  (2, 125)\t0.29026651077075016\n",
      "  (2, 178)\t0.3516583339131581\n",
      "  (2, 262)\t0.20066934295093097\n",
      "  (2, 87)\t0.775140496604744\n",
      "  :\t:\n",
      "  (112, 46)\t0.19511677353623427\n",
      "  (112, 139)\t0.6087393647505877\n",
      "  (113, 63)\t0.061188526342934736\n",
      "  (113, 64)\t0.061188526342934736\n",
      "  (113, 294)\t0.1284109933087752\n",
      "  (113, 278)\t0.13680612774678327\n",
      "  (113, 285)\t0.436267995340527\n",
      "  (113, 171)\t0.436267995340527\n",
      "  (113, 119)\t0.4117105070694453\n",
      "  (113, 310)\t0.570712929272208\n",
      "  (113, 42)\t0.285356464636104\n",
      "  (114, 63)\t0.07061243639514274\n",
      "  (114, 64)\t0.07061243639514274\n",
      "  (114, 247)\t0.23141582771263353\n",
      "  (114, 276)\t0.28036063891239515\n",
      "  (114, 80)\t0.5389513676756498\n",
      "  (114, 127)\t0.19534518584544838\n",
      "  (114, 306)\t0.3089915180691326\n",
      "  (114, 229)\t0.6586109002243137\n",
      "  (115, 63)\t0.09308757247458486\n",
      "  (115, 64)\t0.09308757247458486\n",
      "  (115, 178)\t0.36959624431785965\n",
      "  (115, 264)\t0.4523999098511542\n",
      "  (115, 80)\t0.7104934634748665\n",
      "  (115, 4)\t0.36959624431785965\n"
     ]
    }
   ],
   "source": [
    "# 3. Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the processed descriptions\n",
    "X = vectorizer.fit_transform(df['Processed_Description'])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e88de877-701d-40e1-99df-c2f5ace109f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training successful.\n"
     ]
    }
   ],
   "source": [
    "# 4. Model Training\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "try:\n",
    "    # Initialize the model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, df['Command'])\n",
    "\n",
    "    # If the model trains successfully, print a success message\n",
    "    print(\"Model training successful.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If an error occurs, print the error message\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e164c8be-02cc-43b2-b83c-dd064f68db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your query (type 'exit' to stop):  discover hosts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Nmap command: -Pn\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Was this output correct? Type 'yes', 'no', or 'close':  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged: Query - discover hosts, Predicted Output - -Pn, Feedback - yes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your query (type 'exit' to stop):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n",
      "Program terminated.\n"
     ]
    }
   ],
   "source": [
    "# 5. Prediction\n",
    "\n",
    "def predict_nmap_command(query):\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_text(query)\n",
    "    # Transform the query using the same vectorizer\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    # Predict the command\n",
    "    predicted_command = model.predict(query_vector)\n",
    "    return predicted_command[0]\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Define a function to log user interactions (placeholder implementation)\n",
    "def log_data(user_query, predicted_output, feedback):\n",
    "    print(f\"Logged: Query - {user_query}, Predicted Output - {predicted_output}, Feedback - {feedback}\")\n",
    "\n",
    "# Example pseudocode for the feedback loop\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Please enter your query (type 'exit' to stop): \")\n",
    "    \n",
    "    if user_query.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    \n",
    "    # Use your NLP model to predict an Nmap command based on user_query\n",
    "    predicted_output = predict_nmap_command(user_query)\n",
    "    \n",
    "    print(f\"Predicted Nmap command: {predicted_output}\")\n",
    "    \n",
    "    feedback = input(\"Was this output correct? Type 'yes', 'no', or 'close': \")\n",
    "    \n",
    "    # Process feedback\n",
    "    if feedback == 'yes':\n",
    "        # Reinforce correct prediction in your model\n",
    "        try:\n",
    "            model.fit(X, df['Command'])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during reinforcement: {e}\")\n",
    "            \n",
    "    elif feedback == 'no':\n",
    "        # Update model to avoid similar incorrect predictions\n",
    "        try:\n",
    "            model.fit(X, df['Command'])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during adjustment: {e}\")\n",
    "            \n",
    "    elif feedback == 'close':\n",
    "        # Fine-tune parameters or adjust training data\n",
    "        try:\n",
    "            model.fit(X, df['Command'])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during fine-tuning: {e}\")\n",
    "    \n",
    "    # Log user query, predicted output, and feedback\n",
    "    log_data(user_query, predicted_output, feedback)\n",
    "\n",
    "print(\"Program terminated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "695b85ac-9c06-4b61-a2b8-f12610285a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                          --data <hex string>       0.00      1.00      0.00       0.0\n",
      "                          --disable-keepalive       0.00      1.00      0.00       0.0\n",
      "            --dns-servers <serv1[,serv2],...>       1.00      0.00      0.00       1.0\n",
      "                 --excludefile <exclude_file>       0.00      1.00      0.00       0.0\n",
      "                           --follow-redirects       1.00      0.00      0.00       1.0\n",
      "                        --host-timeout <time>       0.00      1.00      0.00       0.0\n",
      "         --http-auth-cred <username:password>       1.00      0.00      0.00       1.0\n",
      "         --http2-max-concurrent-streams <num>       1.00      0.00      0.00       1.0\n",
      "                --http2-max-frame-size <size>       1.00      0.00      0.00       1.0\n",
      "          --http2-max-header-list-size <size>       0.00      1.00      0.00       0.0\n",
      "                       --ip-options <options>       0.00      1.00      0.00       0.0\n",
      "                              --key <keyfile>       0.00      1.00      0.00       0.0\n",
      "                        --max-redirects <num>       1.00      0.00      0.00       1.0\n",
      "                        --max-retries <tries>       1.00      0.00      0.00       1.0\n",
      "--min-parallelism/max-parallelism <numprobes>       1.00      0.00      0.00       1.0\n",
      "                               --osscan-guess       1.00      0.00      0.00       1.0\n",
      "                               --osscan-limit       0.00      1.00      0.00       0.0\n",
      "                  --proxies <url1,[url2],...>       0.00      1.00      0.00       0.0\n",
      "                                --proxy <url>       1.00      0.00      0.00       1.0\n",
      "             --proxy-auth <username:password>       0.00      1.00      0.00       0.0\n",
      "                                --retry <num>       0.00      1.00      0.00       0.0\n",
      "                         --retry-delay <time>       0.00      1.00      0.00       0.0\n",
      "                        --retry-jitter <time>       1.00      0.00      0.00       1.0\n",
      "                  --script-args-file=filename       1.00      0.00      0.00       1.0\n",
      "            --script-args=<n1=v1,[n2=v2,...]>       0.00      1.00      0.00       0.0\n",
      "                         --send-eth/--send-ip       1.00      0.00      0.00       1.0\n",
      "                           --sslkey <keyfile>       1.00      0.00      0.00       1.0\n",
      "                        --sslproto <protocol>       0.00      1.00      0.00       0.0\n",
      "                                 --system-dns       1.00      0.00      0.00       1.0\n",
      "                         --top-ports <number>       1.00      0.00      0.00       1.0\n",
      "                  --version-intensity <level>       0.00      1.00      0.00       0.0\n",
      "                                           -6       0.00      1.00      0.00       0.0\n",
      "                                           -A       1.00      0.00      0.00       1.0\n",
      "                  -D <decoy1,decoy2[,ME],...>       1.00      0.00      0.00       1.0\n",
      "                                           -F       0.00      1.00      0.00       0.0\n",
      "                              -S <IP_Address>       0.00      1.00      0.00       0.0\n",
      "                                           -V       1.00      0.00      0.00       1.0\n",
      "                   -g/--source-port <portnum>       1.00      0.00      0.00       1.0\n",
      "                                           -h       0.00      1.00      0.00       0.0\n",
      "                          -iL <inputfilename>       1.00      0.00      0.00       1.0\n",
      "                                        -n/-R       1.00      0.00      0.00       1.0\n",
      "                             -p <port ranges>       0.00      1.00      0.00       0.0\n",
      "                                          -sC       0.00      1.00      0.00       0.0\n",
      "                            -sI <zombie host[       1.00      0.00      0.00       1.0\n",
      "                                          -sL       1.00      0.00      0.00       1.0\n",
      "\n",
      "                                     accuracy                           0.00      24.0\n",
      "                                    macro avg       0.53      0.47      0.00      24.0\n",
      "                                 weighted avg       1.00      0.00      0.00      24.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Model Evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['Command'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc82b72-b1bc-462e-9cb2-224e559505d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
