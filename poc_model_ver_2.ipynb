{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3788f5ba-c3b8-470a-8a62-a5834a0fd2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Nmap command: nmap -sU\n",
      "Terminating...\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# Function to load JSON file into a dictionary\n",
    "def load_json_as_dict(json_file_path):\n",
    "    with open(json_file_path, mode='r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "# Load the dataset\n",
    "json_file_path = \"nmap_commands_1.json\"\n",
    "nmap_dataset = load_json_as_dict(json_file_path)\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(list(nmap_dataset.items()), columns=['Command', 'Description'])\n",
    "\n",
    "# Preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, dict):\n",
    "        text = json.dumps(text)  # Convert dict to JSON string if necessary\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "df['Processed_Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Processed_Description'])\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the initial model\n",
    "model.fit(X, df['Command'])\n",
    "\n",
    "# Function to predict and get feedback\n",
    "def predict_nmap_command(query):\n",
    "    processed_query = preprocess_text(query)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    predicted_command = model.predict(query_vector)\n",
    "    return predicted_command[0]\n",
    "\n",
    "def log_feedback(query, predicted_output, feedback):\n",
    "    with open(\"feedback_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Query: {query}\\n\")\n",
    "        f.write(f\"Predicted Output: {predicted_output}\\n\")\n",
    "        f.write(f\"Feedback: {feedback}\\n\")\n",
    "        f.write(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# Example pseudocode for the feedback loop\n",
    "while True:\n",
    "    user_query = input(\"Please enter your query (type 'exit' to terminate): \")\n",
    "    \n",
    "    if user_query.lower() == 'exit':\n",
    "        print(\"Terminating...\")\n",
    "        break\n",
    "    \n",
    "    predicted_output = predict_nmap_command(user_query)\n",
    "    print(f\"Predicted Nmap command: nmap {predicted_output}\")\n",
    "    \n",
    "    feedback = input(\"Was this output correct? Type 'yes', 'no', or 'close': \")\n",
    "    \n",
    "    # Log feedback\n",
    "    log_feedback(user_query, predicted_output, feedback)\n",
    "    \n",
    "    # Update model if feedback is provided\n",
    "    if feedback == 'no':\n",
    "        # Add the current query to the training set\n",
    "        current_query_vector = vectorizer.transform([preprocess_text(user_query)])\n",
    "        X = vstack([X, current_query_vector])\n",
    "        df = pd.concat([df, pd.DataFrame({'Command': [predicted_output], 'Description': [user_query]})], ignore_index=True)\n",
    "        model.fit(X, df['Command'])\n",
    "        print(\"Model updated based on user feedback.\")\n",
    "    \n",
    "print(\"Session ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
